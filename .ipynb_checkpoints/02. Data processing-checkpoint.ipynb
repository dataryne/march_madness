{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in basketball_data are\n",
    " - game_data\n",
    " - team_data\n",
    " - name_xwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "game_data = pd.read_sql_query(\"select * from game_data;\", conn)\n",
    "team_data = pd.read_sql_query(\"select * from team_data;\", conn)\n",
    "name_xwalk = pd.read_sql_query(\"select * from name_xwalk;\", conn)\n",
    "\n",
    "conn.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on the CleanName so all the tables speak to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_xwalk_for_game_data = name_xwalk[['url_name','CleanName','Opp']].drop_duplicates(['CleanName'], keep='first').copy()\n",
    "game_data_clean = pd.merge(game_data,name_xwalk_for_game_data[['url_name','CleanName']],left_on='Tm',right_on='url_name',how='left')\n",
    "game_data_clean.drop(['Tm','url_name','index'],axis=1,inplace=True)\n",
    "game_data_clean=game_data_clean.rename(columns = {'CleanName':'Tm'})\n",
    "game_data_clean = pd.merge(game_data_clean,name_xwalk_for_game_data[['Opp','CleanName']],on='Opp',how='left')\n",
    "game_data_clean.drop(['Opp'],axis=1,inplace=True)\n",
    "game_data_clean=game_data_clean.rename(columns = {'CleanName':'Opp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_xwalk_for_team_data = name_xwalk[['Team','CleanName',]].drop_duplicates(['Team','CleanName'], keep='first').copy()\n",
    "team_data_clean = pd.merge(team_data,name_xwalk[['Team','CleanName']],on='Team',how='left')\n",
    "team_data_clean.drop(['Team','index'],axis=1,inplace=True)\n",
    "team_data_clean=team_data_clean.rename(columns = {'CleanName':'Team'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosswalk my data to Kaggle data  \n",
    "\n",
    "Kaggle provides a lot of data in their March Madness Learning Mania competition. I'd like to include that data with the data I scraped, so I have to do more cleaning of the names before I can merge them. I did most of this manually in excel (see the CrosswalkNames excel file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_names = pd.read_csv(\"Data/Teams.csv\")\n",
    "kaggle_names.Team_Name = kaggle_names.Team_Name.str.upper()\n",
    "kaggle_names.loc[:,'Team_Name_Clean'] = kaggle_names.Team_Name\n",
    "kaggle_names.ix[kaggle_names.Team_Name_Clean == 'W SALEM ST','Team_Name_Clean'] = 'WINSTON SALEM'\n",
    "kaggle_names.Team_Name_Clean.replace(to_replace=\" ST$\",value=\" STATE\",regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_xwalk = pd.read_excel(\"Data/CrosswalkNames.xlsx\")[['Team_Id','Team_Name','Team_Name_Clean','Change to Make']].dropna(axis=0,how=\"all\")\n",
    "kaggle_xwalk = pd.merge(kaggle_names,kaggle_xwalk,on=['Team_Id','Team_Name',\"Team_Name_Clean\"],how=\"outer\")\n",
    "kaggle_xwalk.loc[:,'Team_Name_Clean'] = np.where(kaggle_xwalk['Change to Make'].isnull(),kaggle_xwalk['Team_Name_Clean'],kaggle_xwalk['Change to Make'])\n",
    "kaggle_xwalk.drop('Change to Make',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_xwalk = pd.merge(kaggle_xwalk,name_xwalk,left_on=\"Team_Name_Clean\",right_on=\"CleanName\",how=\"outer\").drop('index',axis=1)\n",
    "name_xwalk.ix[name_xwalk.CleanName.isnull(),'CleanName']=name_xwalk['Team_Name_Clean']\n",
    "name_xwalk.drop('Team_Name_Clean',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following two cells were used to output the table I used to manually build the crosswalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "misses = new_names[((new_names.Team_Id.notnull()) & (new_names.CleanName.isnull())) | ((new_names.Team_Id.isnull()) & (new_names.url_name.notnull()))].copy()\n",
    "misses.loc[:,\"Sorter\"] = np.where(misses.Team_Name_Clean.isnull(),misses.CleanName,misses.Team_Name_Clean)\n",
    "misses = misses.sort_values('Sorter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writer = pd.ExcelWriter('CrosswalkNames.xlsx',engine='xlsxwriter')\n",
    "misses.to_excel(writer,startcol=0,startrow=0,sheet_name='Sheet1',index=False) #index=True will write the index which I almost never want\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Kaggle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seasons - day number, regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Dayzero</th>\n",
       "      <th>Regionw</th>\n",
       "      <th>Regionx</th>\n",
       "      <th>Regiony</th>\n",
       "      <th>Regionz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985</td>\n",
       "      <td>1984-10-29</td>\n",
       "      <td>East</td>\n",
       "      <td>West</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>Southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986</td>\n",
       "      <td>1985-10-28</td>\n",
       "      <td>East</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987</td>\n",
       "      <td>1986-10-27</td>\n",
       "      <td>East</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988</td>\n",
       "      <td>1987-11-02</td>\n",
       "      <td>East</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>Southeast</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989</td>\n",
       "      <td>1988-10-31</td>\n",
       "      <td>East</td>\n",
       "      <td>West</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>Southeast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season    Dayzero Regionw    Regionx    Regiony    Regionz\n",
       "0    1985 1984-10-29    East       West    Midwest  Southeast\n",
       "1    1986 1985-10-28    East    Midwest  Southeast       West\n",
       "2    1987 1986-10-27    East  Southeast    Midwest       West\n",
       "3    1988 1987-11-02    East    Midwest  Southeast       West\n",
       "4    1989 1988-10-31    East       West    Midwest  Southeast"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons = pd.read_csv(\"Data/Seasons.csv\")\n",
    "seasons['Dayzero'] = pd.to_datetime(seasons.Dayzero,format='%m/%d/%Y')\n",
    "seasons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TourneySeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seeds = pd.read_csv(\"Data/TourneySeeds.csv\")\n",
    "seeds.loc[:,'SeedNum'] = seeds.Seed.str[1:3].astype(int)\n",
    "seeds.loc[:,'Region'] = seeds.Seed.str[0]\n",
    "slots = pd.read_csv(\"Data/TourneySlots.csv\")\n",
    "slots.loc[:,'Round'] = slots.Slot.str[1:2].astype(int)\n",
    "#slots.loc[:,'Region'] = np.where(slots.Round < 5,slots.Slot.str[2:3],slots.Slot.str[2:4])\n",
    "#slots.loc[:,'Game'] = np.where(slots.Round < 5,slots.Slot.str[-1],slots.Slot.str[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "season_results_compact = pd.read_csv(\"Data/RegularSeasonCompactResults.csv\")\n",
    "season_results_detailed = pd.read_csv(\"Data/RegularSeasonDetailedResults.csv\")\n",
    "tourney_results_compact = pd.read_csv(\"Data/TourneyCompactResults.csv\")\n",
    "tourney_results_detailed = pd.read_csv(\"Data/TourneyDetailedResults.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some merging/cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add seeds to compact tourney results and change from wteam/lteam to ssteam/wsteam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tourney_results_compact = pd.merge(tourney_results_compact,seeds[['Season','Seed','Team']],left_on=['Season','Wteam'],right_on=['Season','Team'],how='left')\n",
    "tourney_results_compact=tourney_results_compact.rename(columns = {'Seed':'Wteam_Seed'}).drop('Team',axis=1)\n",
    "tourney_results_compact = pd.merge(tourney_results_compact,seeds[['Season','Seed','Team']],left_on=['Season','Lteam'],right_on=['Season','Team'],how='left')\n",
    "tourney_results_compact=tourney_results_compact.rename(columns = {'Seed':'Lteam_Seed'}).drop('Team',axis=1)\n",
    "#create variable that says whether the higher (strong) seed won or lost. The nested where is because in later rounds, you\n",
    "# may get the same seed number when teams from different regions play. In those cases, the region coming first alphabeticall\n",
    "# is considered the strong seed. For example, if X01 and Y01 play, X01 is the strong seed since x < y.\n",
    "tourney_results_compact.loc[:,'StrongSeed'] = np.where(tourney_results_compact.Wteam_Seed.str[1:3] == tourney_results_compact.Lteam_Seed.str[1:3], \n",
    "                                                       np.where(tourney_results_compact.Wteam_Seed <= tourney_results_compact.Lteam_Seed,\"Wteam\",\"Lteam\"),\n",
    "                                                       np.where(tourney_results_compact.Wteam_Seed.str[1:] <= tourney_results_compact.Lteam_Seed.str[1:],\"Wteam\",\"Lteam\"))\n",
    "\n",
    "#send to separate dataframe depending on whether strong seed won\n",
    "tourney_strong_wteam = tourney_results_compact[tourney_results_compact.StrongSeed == \"Wteam\"].copy()\n",
    "tourney_strong_lteam = tourney_results_compact[tourney_results_compact.StrongSeed == \"Lteam\"].copy()\n",
    "#rename columns to SS (strong seed) and WS (weak seed) depending on which won\n",
    "tourney_strong_wteam = tourney_strong_wteam.rename(columns = {'Wteam':'SS_team','Wscore':'SS_score','Lteam':'WS_team',\n",
    "                                                              'Lscore':'WS_score','Wloc':'loc','Wteam_Seed':'SS_Seed',\n",
    "                                                              'Lteam_Seed':'WS_Seed'})\n",
    "tourney_strong_lteam = tourney_strong_lteam.rename(columns = {'Wteam':'WS_team','Wscore':'WS_score','Lteam':'SS_team',\n",
    "                                                              'Lscore':'SS_score','Wloc':'loc','Wteam_Seed':'WS_Seed',\n",
    "                                                              'Lteam_Seed':'SS_Seed'})\n",
    "#put the two dataframes back together. Now instead of wteam and lteam (which we dont know before the game is played), we have SS and WS (which we know\n",
    "#when the bracket is announced)\n",
    "tourney_results_compact = pd.concat([tourney_strong_wteam, tourney_strong_lteam], ignore_index=False).sort_index()\n",
    "#change seed for winner of play-in games (drop a or b from seed)\n",
    "tourney_results_compact.loc[:,'SS_Seed'] = np.where(tourney_results_compact.Daynum >=136,tourney_results_compact.SS_Seed.str[0:3],tourney_results_compact.SS_Seed)\n",
    "tourney_results_compact.loc[:,'WS_Seed'] = np.where(tourney_results_compact.Daynum >=136,tourney_results_compact.WS_Seed.str[0:3],tourney_results_compact.WS_Seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in rest of slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_round_results(previous_round,roundnum):\n",
    "\n",
    "    current_round = slots[(slots.Round ==roundnum) & (slots.Season < 2017)].copy()\n",
    "    current_round_long = pd.merge(pd.melt(current_round,id_vars=['Season','Slot'],value_vars=['Strongseed','Weakseed'],value_name='RSlot'),\n",
    "            previous_round[['Season','Slot','Winning_Seed']],left_on=['Season','RSlot'],right_on=['Season','Slot'],how='left') \n",
    "    current_round_long = current_round_long[['Season','Slot_x','Winning_Seed']].copy()\n",
    "    current_round_long = current_round_long.rename(columns={\"Slot_x\":\"Slot\"})\n",
    "    current_round_long.loc[:,\"SeedNum\"] = current_round_long.Winning_Seed.str[1:3].astype(int)\n",
    "    current_round_long.loc[:,\"Region\"] = current_round_long.Winning_Seed.str[0:1]\n",
    "    current_round_long = current_round_long.sort_values(['Season','Slot','SeedNum','Region']).drop(['SeedNum','Region'],axis=1)\n",
    "    current_round_long.loc[:,'Is_WS'] = current_round_long.groupby(['Season','Slot']).cumcount()\n",
    "    current_round_long_0 = current_round_long[current_round_long.Is_WS == 0].copy().rename(columns={\"Winning_Seed\":\"SS_Seed\"}).drop(['Is_WS'],axis=1)\n",
    "    current_round_long_1 = current_round_long[current_round_long.Is_WS == 1].copy().rename(columns={\"Winning_Seed\":\"WS_Seed\"}).drop(['Is_WS'],axis=1)\n",
    "    current_round = pd.merge(current_round_long_0,current_round_long_1,on=['Season','Slot'])\n",
    "    current_round.loc[:,\"Round\"] = roundnum\n",
    "    return(pd.merge(current_round,tourn_results,on=['Season','SS_Seed','WS_Seed'],how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tourn_results = tourney_results_compact[tourney_results_compact.Season < 2017].copy()\n",
    "tourn_results.loc[:,\"SS_Win\"] = np.where(tourn_results.SS_score >= tourn_results.WS_score,1, 0)\n",
    "tourn_results.loc[:,\"Winning_Seed\"] = np.where(tourn_results.SS_Win ==1,\n",
    "                                               tourn_results.SS_Seed,\n",
    "                                               tourn_results.WS_Seed)\n",
    "tourn_results = tourn_results[['Season','SS_Win','Winning_Seed','SS_Seed','WS_Seed','SS_score','WS_score']].copy()\n",
    "\n",
    "slots_r1 = pd.merge(tourn_results,slots,left_on=['Season','SS_Seed','WS_Seed'],right_on=['Season','Strongseed','Weakseed'],\n",
    "                              how='inner')\n",
    "slots_r2 = get_round_results(slots_r1,2)\n",
    "slots_r3 = get_round_results(slots_r2,3)\n",
    "slots_r4 = get_round_results(slots_r3,4)\n",
    "slots_r5 = get_round_results(slots_r4,5)\n",
    "slots_r6 = get_round_results(slots_r5,6)\n",
    "\n",
    "slots_complete = pd.concat([slots_r1, slots_r2, slots_r3, slots_r4, slots_r5, slots_r6], \n",
    "                           ignore_index=False).reset_index().drop(['index'],axis=1).drop(['Strongseed','Weakseed'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slots_complete = pd.merge(slots_complete,tourney_results_compact[['Season','SS_Seed','WS_Seed','SS_score','WS_score','Daynum','SS_team','WS_team']],\n",
    "         on=['Season','SS_Seed','WS_Seed','SS_score','WS_score'],how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add day number for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game_data_clean['Date'] = pd.to_datetime(game_data_clean.Date,format='%Y-%m-%d')\n",
    "#first_dates = game_data_clean.loc[:,['Year','Date']].groupby('Year').min().reset_index()\n",
    "#first_dates = first_dates.rename(columns = {'Date':'FirstDate'})\n",
    "first_dates = pd.DataFrame()\n",
    "first_dates[['FirstDate','Year']] = seasons[['Dayzero','Season']]\n",
    "game_data_clean = pd.merge(game_data_clean,first_dates,on=\"Year\",how='left')\n",
    "game_data_clean['Day_Number'] = (game_data_clean['Date'] - game_data_clean['FirstDate']).astype('timedelta64[D]').astype(int)\n",
    "game_data_clean.drop('FirstDate',axis=1,inplace=True)\n",
    "game_data_clean.loc[:,\"TournamentGame\"] = np.where(game_data_clean.Day_Number < 134,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe that is all possible matchups in 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slots_2017 = slots[slots.Season == 2017].copy()\n",
    "seeds_2017 = seeds[seeds.Season == 2017].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teams_in_2017 = seeds_2017[[\"Team\",'Seed','SeedNum','Region']].copy()\n",
    "teams_in_2017 = teams_in_2017.reset_index().drop('index',axis=1)\n",
    "teams_in_2017['New_ID'] = 1\n",
    "\n",
    "all_matchups = pd.merge(teams_in_2017.rename(columns={\"Team\":\"Team1\",\"Seed\":\"Seed1\",\"SeedNum\":\"SeedNum1\",\"Region\":\"Region1\"}),\n",
    "                        teams_in_2017.rename(columns={\"Team\":\"Team2\",\"Seed\":\"Seed2\",\"SeedNum\":\"SeedNum2\",\"Region\":\"Region2\"}),\n",
    "                        on='New_ID',\n",
    "                        how='outer').drop('New_ID',axis=1)\n",
    "all_matchups = all_matchups[all_matchups.Seed1 != all_matchups.Seed2]\n",
    "all_matchups.loc[:,'SS_Seed'] = np.where(all_matchups.Seed1.str[1:3] == all_matchups.Seed2.str[1:3], \n",
    "                                                       np.where(all_matchups.Seed1 <= all_matchups.Seed2,\"Seed1\",\"Seed2\"),\n",
    "                                                       np.where(all_matchups.Seed1.str[1:] <= all_matchups.Seed2.str[1:],\"Seed1\",\"Seed2\"))\n",
    "\n",
    "#send to separate dataframe depending on whether strong seed won\n",
    "tourney_strong_Seed1 = all_matchups[all_matchups.SS_Seed == \"Seed1\"].copy().drop(['SeedNum1','Region1','SeedNum2','Region2','SS_Seed'],axis=1)\n",
    "tourney_strong_Seed2 = all_matchups[all_matchups.SS_Seed == \"Seed2\"].copy().drop(['SeedNum1','Region1','SeedNum2','Region2','SS_Seed'],axis=1)\n",
    "#rename columns to SS (strong seed) and WS (weak seed) depending on which won\n",
    "tourney_strong_Seed1 = tourney_strong_Seed1.rename(columns = {'Team1':'SS_team','Team2':'WS_team','Seed1':'SS_Seed','Seed2':'WS_Seed'})\n",
    "tourney_strong_Seed2 = tourney_strong_Seed2.rename(columns = {'Team1':'WS_team','Team2':'SS_team','Seed1':'WS_Seed','Seed2':'SS_Seed'})\n",
    "#put the two dataframes back together. Now instead of Seed1 and Seed2 (which we dont know before the game is played), we have SS and WS (which we know\n",
    "#when the bracket is announced)\n",
    "all_matchups_2017 = pd.concat([tourney_strong_Seed1, tourney_strong_Seed2], ignore_index=False).sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create separate datasets for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2017_game_data = game_data_clean[game_data_clean.Year == 2017].copy()\n",
    "game_data_clean = game_data_clean[game_data_clean.Year < 2017].copy()\n",
    "y2017_team_data = team_data_clean[team_data_clean.Year == 2017].copy()\n",
    "team_data_clean = team_data_clean[team_data_clean.Year < 2017].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save dataframes to SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"Data/clean_basketball_data.db\")\n",
    "slots_2017.to_sql(\"slots_2017\",conn,if_exists=\"replace\")\n",
    "all_matchups_2017.to_sql(\"matchups_2017\",conn,if_exists=\"replace\")\n",
    "y2017_game_data.to_sql(\"game_data_2017\",conn,if_exists=\"replace\")\n",
    "y2017_team_data.to_sql(\"team_data_2017\",conn,if_exists=\"replace\")\n",
    "game_data_clean.to_sql(\"game_data_clean\",conn,if_exists=\"replace\")\n",
    "team_data_clean.to_sql(\"team_data_clean\",conn,if_exists=\"replace\")\n",
    "name_xwalk.to_sql(\"team_names\",conn,if_exists=\"replace\")\n",
    "seasons.to_sql(\"seasons\",conn,if_exists=\"replace\")\n",
    "seeds.to_sql(\"seeds\",conn,if_exists=\"replace\")\n",
    "season_results_compact.to_sql(\"season_results_compact\",conn,if_exists=\"replace\")\n",
    "season_results_detailed.to_sql(\"season_results_detailed\",conn,if_exists=\"replace\")\n",
    "tourney_results_compact.to_sql(\"tourney_results_compact\",conn,if_exists=\"replace\")\n",
    "tourney_results_detailed.to_sql(\"tourney_results_detailed\",conn,if_exists=\"replace\")\n",
    "slots_complete.to_sql(\"tourney_results_complete\",conn,if_exists=\"replace\")\n",
    "slots.to_sql(\"slots\",conn,if_exists=\"replace\")\n",
    "conn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
