{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\computation\\__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info about sql: http://www.sqlitetutorial.net/sqlite-select-distinct  \n",
    "http://stackoverflow.com/questions/20963887/deleting-duplicate-rows-in-sqlite  \n",
    "http://yznotes.com/write-pandas-dataframe-to-sqlite/  \n",
    "https://www.dataquest.io/blog/python-pandas-databases/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years_to_get = range(2013,2018,1) #these are the years that will be scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overview  \n",
    "\n",
    "  \n",
    "This file will scrape data from two websites and put it in a SQLite database. It also does some cleaning of the team names so the data can be matched up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape season efficiency numbers from Ken Pomeroy's Website  \n",
    "\n",
    "http://kenpom.com/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to get table from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_table_from_url(souper):\n",
    "    \n",
    "    tables = souper.find_all(\"table\")\n",
    "    table_list=[]\n",
    "    for table in tables:\n",
    "        #Get all rows\n",
    "        rows = table.find_all(\"tr\")\n",
    "        #Loop over rows\n",
    "        list_of_rows = []\n",
    "        for row in rows:\n",
    "            #Get all columns\n",
    "            cols = row.find_all(\"td\")\n",
    "            #Loop over columns\n",
    "            single_row = []\n",
    "            for col in cols:\n",
    "                z = col.get_text()\n",
    "                #Append column text to single_row list\n",
    "                single_row.append(z)\n",
    "            #Append row (list of columns) to list_of_rows list\n",
    "            list_of_rows.append(single_row)\n",
    "        #List of rows is now a lists of lists. Convert to df\n",
    "        df = pd.DataFrame(list_of_rows)\n",
    "        table_list.append(df)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to get the data and do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_the_url(url):\n",
    "    \n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "\n",
    "    return(get_table_from_url(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  The next two cells are commented out in favor os using an alternative method that pulls from waybackmachine rather than directly from Pomeroy's site. They are still included here though for future reference, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_season_data(year):\n",
    "    \n",
    "    if year == 2017:\n",
    "        url = \"http://kenpom.com/index.php\"\n",
    "    else:\n",
    "        url = \"http://kenpom.com/index.php?y=\"+str(year)\n",
    "    \n",
    "    df = grab_the_url(url)\n",
    "\n",
    "    colnames = [\"Rank\",\"Team\",\"Conf\",\"W-L\",\"AdjEM\",\"AdjO\",\"AdjO_Rank\",\"AdjD\",\"AdjD_Rank\",\"AdjT\",\"AdjT_Rank\",\"Luck\",\"Luck_Rank\",\"SOSAdjEM\",\"SOSAdjEM_Rank\",\"OppO\",\"OppO_Rank\",\"OppD\",\"OppD_Rank\",\"NCSOSAdjEM\",\"NCSOSAdjEM_Rank\"]\n",
    "    df.columns = colnames\n",
    "    df.dropna(axis=0,thresh=10,inplace=True) # #remove the two empty rows at top\n",
    "    df['Team'].replace(to_replace=\"\\d\",value=\"\",inplace=True,regex=True)\n",
    "    df.loc[:,\"Team\"] = df['Team'].str.strip()\n",
    "    df = df.join(df['W-L'].str.split('-',1,expand=True).rename(columns={0:'Wins',1:'Losses'}))\n",
    "    df.drop('W-L',axis=1,inplace=True)\n",
    "    df = df.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    df.loc[:,\"Year\"] = year\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "\n",
    "season_data = {}\n",
    "for year in years_to_get:\n",
    "    #season_data[year] = get_season_data(year)\n",
    "    #get_season_data(year).to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "\n",
    "conn.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put 2013 to 2017 season level data in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_season_data_alternateway(year,url):\n",
    "    \n",
    "  \n",
    "    df = grab_the_url(url)\n",
    "\n",
    "    colnames = [\"Rank\",\"Team\",\"Conf\",\"W-L\",\"AdjEM\",\"AdjO\",\"AdjO_Rank\",\"AdjD\",\"AdjD_Rank\",\"AdjT\",\"AdjT_Rank\",\"Luck\",\"Luck_Rank\",\"SOSAdjEM\",\"SOSAdjEM_Rank\",\"OppO\",\"OppO_Rank\",\"OppD\",\"OppD_Rank\",\"NCSOSAdjEM\",\"NCSOSAdjEM_Rank\"]\n",
    "    df.columns = colnames\n",
    "    df.dropna(axis=0,thresh=10,inplace=True) # #remove the two empty rows at top\n",
    "    df['Team'].replace(to_replace=\"\\d\",value=\"\",inplace=True,regex=True)\n",
    "    df.loc[:,\"Team\"] = df['Team'].str.strip()\n",
    "    df = df.join(df['W-L'].str.split('-',1,expand=True).rename(columns={0:'Wins',1:'Losses'}))\n",
    "    df.drop('W-L',axis=1,inplace=True)\n",
    "    df = df.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    df.loc[:,\"Year\"] = year\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection Sunday Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these as reference so I know what archive to pull from waybackmachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-12</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-15</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-16</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-03-17</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-03-11</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2011-03-13</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SS  Year\n",
       "0 2017-03-12  2017\n",
       "1 2016-03-13  2016\n",
       "2 2015-03-15  2015\n",
       "3 2014-03-16  2014\n",
       "4 2013-03-17  2013\n",
       "5 2012-03-11  2012\n",
       "6 2011-03-13  2011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = [2017,2016,2015,2014,2013,2012,2011]\n",
    "ss = [\"2017-03-12\",\"2016-03-13\",\"2015-03-15\",\"2014-03-16\",\"2013-03-17\",\"2012-03-11\",\"2011-03-13\"]\n",
    "selection_sunday = pd.DataFrame({\"Year\": year,\"SS\": ss})\n",
    "selection_sunday['SS'] = pd.to_datetime(selection_sunday.SS,format='%Y-%m-%d')\n",
    "selection_sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ken Pomeroy updates his season numbers through the championship game, so I need to use the waybackmachine to get numbers prior to the tournament. I use the last available archive after selection sunday and before the tournament starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "get_season_data_alternateway(2017,\"http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"replace\")\n",
    "get_season_data_alternateway(2016,\"https://web.archive.org/web/20160313185626/http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "get_season_data_alternateway(2015,\"https://web.archive.org/web/20150317134006/http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "get_season_data_alternateway(2014,\"https://web.archive.org/web/20140319032435/http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "get_season_data_alternateway(2013,\"https://web.archive.org/web/20130316035756/http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "get_season_data_alternateway(2012,\"https://web.archive.org/web/20120315142211/http://kenpom.com/index.php\").to_sql(\"team_data\",conn,if_exists=\"append\")\n",
    "\n",
    "conn.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape game level data from Sports-Reference.com  \n",
    "http://www.sports-reference.com/cbb/schools/brigham-young/2017-gamelogs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scrape and clean data for a team in a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_season_for_team(team,year):\n",
    "    url = \"http://www.sports-reference.com/cbb/schools/\" + team + \"/\" + str(year) + \"-gamelogs.html\"\n",
    "    df = grab_the_url(url)\n",
    "    \n",
    "    \n",
    "    # add column names\n",
    "    df.columns = ['Date','Location','Opp','Win','Tm_Pts','Opp_Pts','Tm_FG','Tm_FGA','Tm_FG_pct','Tm_3P','Tm_3PA','Tm_3P_pct','Tm_FT','Tm_FTA','Tm_FT_pct','Tm_ORB','Tm_TRB','Tm_AST','Tm_STL','Tm_BLK','Tm_TOV','Tm_PF','','Opp_FG','Opp_FGA','Opp_FG_pct','Opp_3P','Opp_3PA','Opp_3P_pct','Opp_FT','Opp_FTA','Opp_FT_pct','Opp_ORB','Opp_TRB','Opp_AST','Opp_STL','Opp_BLK','Opp_TOV','Opp_PF']\n",
    "    # remove rows that are blank\n",
    "    df.dropna(axis=0,thresh=10,inplace=True)\n",
    "    df.Win.replace(to_replace=\"\\(\\d OT\\)\",value=\"\",inplace=True,regex=True)\n",
    "    df.Win = df.Win.str.strip()\n",
    "    df.loc[df.Win==\"W\",'Win'] = 1\n",
    "    df.loc[df.Win==\"L\",'Win'] = 0\n",
    "    df.loc[df.Location==\"\",'Location'] = \"Home\"\n",
    "    df.loc[df.Location==\"N\",'Location'] = \"Neutral\"\n",
    "    df.loc[df.Location==\"@\",'Location'] = \"Away\"\n",
    "    df.drop('',axis=1,inplace=True)\n",
    "    df = df.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\n",
    "    df.loc[:,\"Tm\"] = team\n",
    "    df.loc[:,\"Year\"] = year\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#year = 2017\n",
    "#columns_for_big_df = list(get_season_for_team(\"brigham-young\",2017).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of all current schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>School</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilene Christian Wildcats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air Force Falcons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron Zips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama A&amp;M Bulldogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama Crimson Tide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       School\n",
       "1  Abilene Christian Wildcats\n",
       "2           Air Force Falcons\n",
       "3                  Akron Zips\n",
       "4        Alabama A&M Bulldogs\n",
       "5        Alabama Crimson Tide"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schools_page_url = \"http://www.sports-reference.com/cbb/schools/\"\n",
    "\n",
    "schools_to_include = grab_the_url(schools_page_url)\n",
    "\n",
    "# remove rows that are blank\n",
    "schools_to_include.dropna(axis=0,thresh=10,inplace=True)\n",
    "#df.drop('',axis=1,inplace=True)\n",
    "schools_to_include = schools_to_include.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "schools_to_include = schools_to_include.iloc[:,0:4]\n",
    "schools_to_include.columns = ['School','Location','From','To']\n",
    "schools_to_include = schools_to_include[schools_to_include.To > 2006] #only keep teams that are still D1\n",
    "schools_to_include = schools_to_include[['School']]\n",
    "schools_to_include.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get url and name for all schools and then subset to only current D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = urllib.urlopen(schools_page_url).read()\n",
    "names_soup = BeautifulSoup(html,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "school_urls = []\n",
    "for link in names_soup.findAll('a', href=True):\n",
    "    #print link['href']\n",
    "    #all_links.append(link['href'])\n",
    "    try:\n",
    "        m = re.search(\"(?<=/cbb/schools/)[a-z-]+/$\",link['href']).group(0)\n",
    "        #school_urls.append([m,link.get_text()])\n",
    "        school_urls.append([m[:len(m)-1],link.get_text()])\n",
    "#        print link.get_text()\n",
    "    except:\n",
    "        continue\n",
    "schools_df = pd.DataFrame(school_urls,columns=[\"url_name\",\"School\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subset to only schools that are currently division 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schools = pd.merge(schools_df,schools_to_include,on=\"School\",how=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape yearly game data for every D1 team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schools.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_info_for_chunk(schools,error_list):\n",
    "    for school in schools['url_name']:\n",
    "        for year in years_to_get:\n",
    "            try:\n",
    "                get_season_for_team(school,year).to_sql(\"game_data\",conn,if_exists=\"append\")\n",
    "            except:\n",
    "                print \"could not get \"+school+str(year)\n",
    "                error_list.append([school,year])\n",
    "\n",
    "        print \"finished \"+school\n",
    "    return(error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this for getting data for multiple years (currently commented out so dont overwrite database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "year = [2016,2015,2014,2013,2012,2011] #2017 was not included since the regular season wasn't finished yet.\n",
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "game_level_data = {}\n",
    "\n",
    "error_list = []\n",
    "\n",
    "error_list = get_info_for_chunk(schools,error_list)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this for getting the 2017 data once the regular season ended (currently commented out so dont overwrite database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "error_list = []\n",
    "for school in schools['url_name']:\n",
    "    try:\n",
    "        get_season_for_team(school,2017).to_sql(\"game_data\",conn,if_exists=\"append\")\n",
    "    except:\n",
    "        print \"could not get \"+school+str(2017)\n",
    "        error_list.append([school,2017])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use this to look at any errors, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#error_list = pd.DataFrame(error_list,columns=['School','Year'])\n",
    "#error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crosswalk for names  \n",
    "\n",
    "The names are different between the two data sources. All that follows is for the purpose of matching up the names so can merge the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make changes to names pulled from Sports Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schools.loc[:,\"CleanName\"] = schools.url_name\n",
    "schools.CleanName.replace(to_replace=\"-\",value=\" \",inplace=True,regex=True)\n",
    "schools.CleanName = schools.CleanName.str.upper()\n",
    "schools.ix[schools.CleanName == \"CENTRAL CONNECTICUT STATE\",'CleanName'] = \"CENTRAL CONNECTICUT\"  \n",
    "schools.ix[schools.CleanName == \"CENTENARY LA\",'CleanName'] = \"CENTENARY\"\n",
    "schools.ix[schools.CleanName == \"DETROIT MERCY\",'CleanName'] = \"DETROIT\"\n",
    "schools.ix[schools.CleanName == \"BOWLING GREEN STATE\",'CleanName'] = \"BOWLING GREEN\"\n",
    "schools.ix[schools.CleanName == \"NEVADA LAS VEGAS\",'CleanName'] = \"UNLV\"\n",
    "schools.ix[schools.CleanName == \"VIRGINIA COMMONWEALTH\",'CleanName'] = \"VCU\"\n",
    "schools.ix[schools.CleanName == \"TEXAS CHRISTIAN\",'CleanName'] = \"TCU\"\n",
    "schools.ix[schools.CleanName == \"SOUTHERN CALIFORNIA\",'CleanName'] = \"USC\"\n",
    "schools.ix[schools.CleanName == \"SOUTH CAROLINA UPSTATE\",'CleanName'] = \"USC UPSTATE\"\n",
    "schools.ix[schools.CleanName == \"TEXAS SAN ANTONIO\",'CleanName'] = \"UTSA\"\n",
    "schools.ix[schools.CleanName == \"TEXAS EL PASO\",'CleanName'] = \"UTEP\"\n",
    "schools.ix[schools.CleanName == \"SOUTHERN METHODIST\",'CleanName'] = \"SMU\"\n",
    "schools.ix[schools.CleanName == \"LOUISIANA STATE\",'CleanName'] = \"LSU\"\n",
    "schools.ix[schools.CleanName == \"SAINT MARYS CA\",'CleanName'] = \"SAINT MARYS\"\n",
    "schools.ix[schools.CleanName == \"MARYLAND BALTIMORE COUNTY\",'CleanName'] = \"UMBC\"\n",
    "schools.ix[schools.CleanName == \"LOYOLA IL\", 'CleanName'] = \"LOYOLA CHICAGO\"\n",
    "schools.ix[schools.CleanName == \"PENNSYLVANIA\", 'CleanName'] = \"PENN\"\n",
    "schools.ix[schools.CleanName == \"SAINT FRANCIS PA\",\"CleanName\"] = \"ST FRANCIS PA\"\n",
    "schools.ix[schools.CleanName == \"MISSOURI KANSAS CITY\",\"CleanName\"] = \"UMKC\"\n",
    "schools.ix[schools.CleanName == \"MASSACHUSETTS LOWELL\",\"CleanName\"] = \"UMASS LOWELL\"\n",
    "schools.ix[schools.CleanName == \"TEXAS ARLINGTON\",\"CleanName\"] = \"UT ARLINGTON\"\n",
    "sportsref_names = schools.copy()\n",
    "#sportsref_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data scraped from Pomeroy and clean the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "team_data = pd.read_sql_query(\"select * from team_data;\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_data.loc[:,\"CleanName\"] = team_data.Team\n",
    "team_data.CleanName = team_data.CleanName.str.upper()\n",
    "team_data.CleanName.replace(to_replace=\"\\&\",value=\"\",inplace=True,regex=True)\n",
    "team_data.CleanName.replace(to_replace=\"\\'\",value=\"\",inplace=True,regex=True)\n",
    "team_data.ix[team_data.CleanName == \"ST. JOHNS\",\"CleanName\"] = \"ST JOHNS NY\"\n",
    "team_data.CleanName.replace(to_replace=\" ST\\.\",value=\" STATE\",inplace=True,regex=True)\n",
    "team_data.CleanName.replace(to_replace=\"UC \",value=\"CALIFORNIA \",inplace=True,regex=True)\n",
    "team_data.CleanName.replace(to_replace=\"UNC \",value=\"NORTH CAROLINA \",inplace=True,regex=True)\n",
    "team_data.CleanName.replace(to_replace=\"NC \",value=\"NORTH CAROLINA \",inplace=True,regex=True)\n",
    "team_data.ix[team_data.CleanName == \"BYU\",'CleanName'] = \"BRIGHAM YOUNG\"\n",
    "team_data.ix[team_data.CleanName == \"ALBANY\",'CleanName'] = \"ALBANY NY\"\n",
    "team_data.ix[team_data.CleanName == \"TEXAS AM CORPUS CHRIS\",'CleanName'] = \"TEXAS AM CORPUS CHRISTI\"\n",
    "team_data.ix[team_data.CleanName == \"GRAMBLING STATE\",'CleanName'] = \"GRAMBLING\"\n",
    "team_data.ix[team_data.CleanName == \"FIU\",'CleanName'] = \"FLORIDA INTERNATIONAL\"\n",
    "team_data.ix[team_data.CleanName == \"LIU BROOKLYN\",'CleanName'] = \"LONG ISLAND UNIVERSITY\"\n",
    "team_data.ix[team_data.CleanName == \"LONG ISLAND\",'CleanName'] = \"LONG ISLAND UNIVERSITY\"\n",
    "team_data.ix[team_data.CleanName == \"STEPHEN F. AUSTIN\",'CleanName'] = \"STEPHEN F AUSTIN\"\n",
    "team_data.ix[team_data.CleanName == \"SOUTHERN MISS\",'CleanName'] = \"SOUTHERN MISSISSIPPI\"\n",
    "team_data.ix[team_data.CleanName == \"SIU EDWARDSVILLE\",'CleanName'] = \"SOUTHERN ILLINOIS EDWARDSVILLE\"\n",
    "team_data.ix[team_data.CleanName == \"VMI\", 'CleanName'] = \"VIRGINIA MILITARY INSTITUTE\"\n",
    "team_data.ix[team_data.CleanName == \"UAB\", 'CleanName'] = \"ALABAMA BIRMINGHAM\"\n",
    "team_data.ix[team_data.CleanName == \"UCF\", 'CleanName'] = \"CENTRAL FLORIDA\"\n",
    "team_data.ix[team_data.CleanName == \"THE CITADEL\", 'CleanName'] = \"CITADEL\"\n",
    "team_data.ix[team_data.CleanName == \"PRAIRIE VIEW AM\", 'CleanName'] = \"PRAIRIE VIEW\"\n",
    "team_data.ix[team_data.CleanName == \"MOUNT STATE MARYS\",\"CleanName\"] = \"MOUNT ST MARYS\"\n",
    "team_data.ix[team_data.CleanName == \"ST JOHNS\",\"CleanName\"] = \"ST JOHNS NY\"\n",
    "team_data.ix[team_data.CleanName == \"WILLIAM  MARY\",\"CleanName\"] = \"WILLIAM MARY\"\n",
    "team_data.ix[team_data.CleanName == \"LITTLE ROCK\",\"CleanName\"] = \"ARKANSAS LITTLE ROCK\"\n",
    "team_data.ix[team_data.CleanName == \"UT RIO GRANDE VALLEY\",\"CleanName\"] = \"TEXAS PAN AMERICAN\"\n",
    "team_data.CleanName.replace(to_replace=\"\\.\",value=\"\",inplace=True,regex=True)\n",
    "team_data.ix[team_data.CleanName == \"FORT WAYNE\",\"CleanName\"] = \"IUPU FORT WAYNE\"\n",
    "team_data.ix[team_data.CleanName == \"LOUISIANA STATE\",\"CleanName\"] = \"LSU\"\n",
    "team_data.ix[team_data.CleanName == \"MD BALTIMORE COUNTY\",\"CleanName\"] = \"UMBC\"\n",
    "team_data.ix[team_data.CleanName == \"MISSOURI KANSAS CITY\",\"CleanName\"] = \"UMKC\"\n",
    "team_data.ix[team_data.CleanName == \"NEVADA LAS VEGAS\",\"CleanName\"] = \"UNLV\"\n",
    "team_data.ix[team_data.CleanName == \"NJ INST OF TECHNOLOGY\",\"CleanName\"] = \"NJIT\"\n",
    "team_data.ix[team_data.CleanName == \"PENNSYLVANIA\",\"CleanName\"] = \"PENN\"\n",
    "team_data.ix[team_data.CleanName == \"SOUTH CAROLINA UPSTATE\",\"CleanName\"] = \"USC UPSTATE\"\n",
    "team_data.ix[team_data.CleanName == \"SOUTHERN CALIFORNIA\",\"CleanName\"] = \"USC\"\n",
    "team_data.ix[team_data.CleanName == \"SOUTHERN METHODIST\",\"CleanName\"] = \"SMU\"\n",
    "team_data.ix[team_data.CleanName == \"TEXAS ARLINGTON\",\"CleanName\"] = \"UT ARLINGTON\"\n",
    "team_data.ix[team_data.CleanName == \"TEXAS CHRISTIAN\",\"CleanName\"] = \"TCU\"\n",
    "team_data.ix[team_data.CleanName == \"TEXAS EL PASO\",\"CleanName\"] = \"UTEP\"\n",
    "team_data.ix[team_data.CleanName == \"TEXAS SAN ANTONIO\",\"CleanName\"] = \"UTSA\"\n",
    "team_data.ix[team_data.CleanName == \"VIRGINIA COMMONWEALTH\",\"CleanName\"] = \"VCU\"\n",
    "team_data.ix[team_data.CleanName == \"VIRGINIA MILITARY INST\",\"CleanName\"] = \"VIRGINIA MILITARY INSTITUTE\"\n",
    "team_data.ix[team_data.CleanName == \"ST LOUIS\",\"CleanName\"] = \"SAINT LOUIS\"\n",
    "team_data.ix[team_data.CleanName == \"ST MARYS\",\"CleanName\"] = \"SAINT MARYS\"\n",
    "team_data.ix[team_data.CleanName == \"ST PETERS\",\"CleanName\"] = \"SAINT PETERS\"\n",
    "team_data.ix[team_data.CleanName == \"WISCONSIN GREEN BAY\",\"CleanName\"] = \"GREEN BAY\"\n",
    "team_data.ix[team_data.CleanName == \"WISCONSIN MILWAUKEE\",\"CleanName\"] = \"MILWAUKEE\"\n",
    "\n",
    "team_data.sort_values('CleanName',inplace=True)\n",
    "pomeroy_names = team_data[['Team','CleanName']].drop_duplicates(['Team','CleanName'], keep='first').copy()\n",
    "\n",
    "#pomeroy_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the sports ref and pomeroy names to create a crosswalk for merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_crosswalk = pd.merge(sportsref_names,pomeroy_names,on='CleanName',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Opp names in the game data and then add it to the names crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "game_data = pd.read_sql_query(\"select * from game_data;\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'Date', 'Location', 'Opp', 'Win', 'Tm_Pts', 'Opp_Pts',\n",
       "       'Tm_FG', 'Tm_FGA', 'Tm_FG_pct', 'Tm_3P', 'Tm_3PA', 'Tm_3P_pct',\n",
       "       'Tm_FT', 'Tm_FTA', 'Tm_FT_pct', 'Tm_ORB', 'Tm_TRB', 'Tm_AST',\n",
       "       'Tm_STL', 'Tm_BLK', 'Tm_TOV', 'Tm_PF', 'Opp_FG', 'Opp_FGA',\n",
       "       'Opp_FG_pct', 'Opp_3P', 'Opp_3PA', 'Opp_3P_pct', 'Opp_FT',\n",
       "       'Opp_FTA', 'Opp_FT_pct', 'Opp_ORB', 'Opp_TRB', 'Opp_AST', 'Opp_STL',\n",
       "       'Opp_BLK', 'Opp_TOV', 'Opp_PF', 'Tm', 'Year'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_opp_names = game_data[['Opp']].drop_duplicates(keep='first')\n",
    "\n",
    "testing_opp_names.loc[:,\"CleanName\"] = testing_opp_names.Opp\n",
    "testing_opp_names.CleanName.replace(to_replace=\"-\",value=\" \",inplace=True,regex=True)\n",
    "testing_opp_names.CleanName = testing_opp_names.CleanName.str.upper()\n",
    "testing_opp_names.CleanName.replace(to_replace=\"\\&\",value=\"\",inplace=True,regex=True)\n",
    "testing_opp_names.CleanName.replace(to_replace=\"\\'\",value=\"\",inplace=True,regex=True)\n",
    "testing_opp_names.CleanName.replace(to_replace=\"[\\(\\)]\",value=\"\",inplace=True,regex=True)\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"ST. JOHNS\",\"CleanName\"] = \"ST JOHNS NY\"\n",
    "testing_opp_names.CleanName.replace(to_replace=\" ST\\.\",value=\" STATE\",inplace=True,regex=True)\n",
    "testing_opp_names.CleanName.replace(to_replace=\"UC \",value=\"CALIFORNIA \",inplace=True,regex=True)\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"CENTRAL CONNECTICUT STATE\",'CleanName'] = \"CENTRAL CONNECTICUT\"  \n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"CENTENARY LA\",'CleanName'] = \"CENTENARY\"\n",
    "#testing_opp_names.ix[testing_opp_names.CleanName == \"ALBANY NY\",'CleanName'] = \"ALBANY\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UCF\", 'CleanName'] = \"CENTRAL FLORIDA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UNC\", 'CleanName'] = \"NORTH CAROLINA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UCONN\", 'CleanName'] = \"CONNECTICUT\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"DETROIT MERCY\",'CleanName'] = \"DETROIT\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"BOWLING GREEN STATE\",'CleanName'] = \"BOWLING GREEN\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"NEVADA LAS VEGAS\",'CleanName'] = \"UNLV\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"VIRGINIA COMMONWEALTH\",'CleanName'] = \"VCU\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"TEXAS CHRISTIAN\",'CleanName'] = \"TCU\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SOUTHERN CALIFORNIA\",'CleanName'] = \"USC\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SOUTH CAROLINA UPSTATE\",'CleanName'] = \"USC UPSTATE\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"TEXAS SAN ANTONIO\",'CleanName'] = \"UTSA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"TEXAS EL PASO\",'CleanName'] = \"UTEP\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SOUTHERN METHODIST\",'CleanName'] = \"SMU\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"LOUISIANA STATE\",'CleanName'] = \"LSU\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SAINT MARYS CA\",'CleanName'] = \"SAINT MARYS\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"MARYLAND BALTIMORE COUNTY\",'CleanName'] = \"UMBC\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"LOYOLA IL\", 'CleanName'] = \"LOYOLA CHICAGO\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"PENNSYLVANIA\", 'CleanName'] = \"PENN\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SAINT FRANCIS PA\",\"CleanName\"] = \"ST FRANCIS PA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"MISSOURI KANSAS CITY\",\"CleanName\"] = \"UMKC\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UMASS\",\"CleanName\"] = \"MASSACHUSETTS\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"MASSACHUSETTS LOWELL\",\"CleanName\"] = \"UMASS LOWELL\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"TEXAS ARLINGTON\",\"CleanName\"] = \"UT ARLINGTON\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"LOYOLA (IL)\",\"CleanName\"] = \"LOYOLA CHICAGO\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"LOYOLA (MD)\",\"CleanName\"] = \"LOYOLA MD\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"BYU\",'CleanName'] = \"BRIGHAM YOUNG\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UNIVERSITY OF CALIFORNIA\",'CleanName'] = \"CALIFORNIA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UCSB\",'CleanName'] = \"CALIFORNIA SANTA BARBARA\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"TEXAS RIO GRANDE VALLEY\",'CleanName'] = \"TEXAS PAN AMERICAN\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"ETSU\",'CleanName'] = \"EAST TENNESSEE STATE\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UIC\", 'CleanName'] = \"ILLINOIS CHICAGO\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"PITT\", 'CleanName'] = \"PITTSBURGH\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"OLE MISS\", 'CleanName'] = \"MISSISSIPPI\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"LIU BROOKLYN\",'CleanName'] = \"LONG ISLAND UNIVERSITY\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"MOUNT STATE MARYS\",\"CleanName\"] = \"MOUNT ST MARYS\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"SOUTHERN MISS\",'CleanName'] = \"SOUTHERN MISSISSIPPI\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"UT MARTIN\",'CleanName'] = \"TENNESSEE MARTIN\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"ST. JOSEPHS\",'CleanName'] = \"SAINT JOSEPHS\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"ST. PETERS\",'CleanName'] = \"SAINT PETERS\"\n",
    "testing_opp_names.ix[testing_opp_names.CleanName == \"WILLIAM  MARY\",'CleanName'] = \"WILLIAM MARY\"\n",
    "testing_opp_names.CleanName.replace(to_replace=\"\\.\",value=\"\",inplace=True,regex=True)\n",
    "\n",
    "\n",
    "full_xwalk = pd.merge(testing_opp_names,names_crosswalk,on='CleanName',how='outer') #there are schools that show up in Opp\n",
    "# but not url_name or Team. There is no game or team level data available for these teams.\n",
    "full_xwalk = full_xwalk[['CleanName','url_name','Opp','Team']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"basketball_data.db\")\n",
    "full_xwalk.to_sql(\"name_xwalk\",conn,if_exists=\"replace\")\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
